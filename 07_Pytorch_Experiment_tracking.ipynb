{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DietzscheNostoevsky/Learning_Pytorch/blob/main/07_Pytorch_Experiment_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Experiment Tracking"
      ],
      "metadata": {
        "id": "uEAMjuVlNk9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone Project 1: FoodVision Mini Experiment Tracking."
      ],
      "metadata": {
        "id": "4EF6xWSiFSzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup"
      ],
      "metadata": {
        "id": "eO72C58VIu-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWdWaMsGIw41",
        "outputId": "26c9c964-ebcd-47ec-849d-434b5e42a540"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "torch version: 2.0.1+cu118\n",
            "torchvision version: 0.15.2+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mqf_sDYJE17",
        "outputId": "43861b82-4590-496e-886f-cd385c0e509e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 3808, done.\u001b[K\n",
            "remote: Counting objects: 100% (451/451), done.\u001b[K\n",
            "remote: Compressing objects: 100% (268/268), done.\u001b[K\n",
            "remote: Total 3808 (delta 234), reused 354 (delta 176), pack-reused 3357\u001b[K\n",
            "Receiving objects: 100% (3808/3808), 650.23 MiB | 16.87 MiB/s, done.\n",
            "Resolving deltas: 100% (2190/2190), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "URquB3YTJOvX",
        "outputId": "61020f60-7850-4cfb-f0a5-9a60db3e13bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds\n",
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Random seed to set. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Set the seed for general torch operations\n",
        "    torch.manual_seed(seed)\n",
        "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "yyxRf5q5JYmx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Getting Data"
      ],
      "metadata": {
        "id": "SjEGO06aJjOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import zipfile \n",
        "from pathlib import Path\n",
        "import requests \n",
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "  Args:\n",
        "    dir_path (str or pathlib.Path): target directory\n",
        "  \n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "_fzZey8PKiZg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smaller Dataset"
      ],
      "metadata": {
        "id": "AsjI9pR2KCTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_data(source: str, \n",
        "                  destination: str,\n",
        "                  remove_source: bool = True) -> Path:\n",
        "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n",
        "\n",
        "    Args:\n",
        "        source (str): A link to a zipped file containing data.\n",
        "        destination (str): A target directory to unzip data to.\n",
        "        remove_source (bool): Whether to remove the source after downloading and extracting.\n",
        "    \n",
        "    Returns:\n",
        "        pathlib.Path to downloaded data.\n",
        "    \n",
        "    Example usage:\n",
        "        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                      destination=\"pizza_steak_sushi\")\n",
        "    \"\"\"\n",
        "    # Setup path to data folder\n",
        "    data_path = Path(\"data/\")\n",
        "    image_path = data_path / destination\n",
        "\n",
        "    # If the image folder doesn't exist, download it and prepare it... \n",
        "    if image_path.is_dir():\n",
        "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
        "        image_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Download pizza, steak, sushi data\n",
        "        target_file = Path(source).name\n",
        "        with open(data_path / target_file, \"wb\") as f:\n",
        "            request = requests.get(source)\n",
        "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
        "            f.write(request.content)\n",
        "\n",
        "        # Unzip pizza, steak, sushi data\n",
        "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
        "            print(f\"[INFO] Unzipping {target_file} data...\") \n",
        "            zip_ref.extractall(image_path)\n",
        "\n",
        "        # Remove .zip file\n",
        "        if remove_source:\n",
        "            os.remove(data_path / target_file)\n",
        "    \n",
        "    return image_path\n",
        "\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY8jJbPfJl5d",
        "outputId": "85be9c67-1099-4ebc-8a8e-dd2b2fec773d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Lh8EzcKoVq",
        "outputId": "12c3b347-4e43-42a8-c577-d46a06468121"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in 'data/pizza_steak_sushi'.\n",
            "There are 3 directories and 0 images in 'data/pizza_steak_sushi/test'.\n",
            "There are 0 directories and 19 images in 'data/pizza_steak_sushi/test/steak'.\n",
            "There are 0 directories and 25 images in 'data/pizza_steak_sushi/test/pizza'.\n",
            "There are 0 directories and 31 images in 'data/pizza_steak_sushi/test/sushi'.\n",
            "There are 3 directories and 0 images in 'data/pizza_steak_sushi/train'.\n",
            "There are 0 directories and 75 images in 'data/pizza_steak_sushi/train/steak'.\n",
            "There are 0 directories and 78 images in 'data/pizza_steak_sushi/train/pizza'.\n",
            "There are 0 directories and 72 images in 'data/pizza_steak_sushi/train/sushi'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Larger Dataset"
      ],
      "metadata": {
        "id": "XLuzSb4CKu-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting larger dataset from Drive"
      ],
      "metadata": {
        "id": "fcZRItsEMlh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EJ5NLsqyOHfa",
        "outputId": "1ed1234f-d1ae-4b77-ec46-9affd3e7d761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = Path(\"data/\") # The trailing forward slash (/) in the string \n",
        "                          # is used to indicate that it represents a \n",
        "                          # directory rather than a specific file. \n",
        "                          # It's a common convention to include the trailing slash \n",
        "                          # in directory paths to differentiate them from file paths.\n",
        "\n",
        "image_path_full = data_path / \"pizza_steak_sushi_full\"\n",
        "\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path_full} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path_full.mkdir(parents=True, exist_ok=True)\n",
        "zip_data = \"/content/drive/Othercomputers/My MacBook Air/GitHub/-Machine_Learning/Learning_Pytorch/pizza_steak_sushi_100_percent.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_data, \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\") \n",
        "        zip_ref.extractall(image_path_full)\n",
        "walk_through_dir(image_path_full)"
      ],
      "metadata": {
        "id": "TYUwn8trMqDV",
        "outputId": "b5a61c27-20d2-4519-bbfe-4b93487bb1b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi_full directory exists.\n",
            "Unzipping pizza, steak, sushi data...\n",
            "There are 2 directories and 0 images in 'data/pizza_steak_sushi_full'.\n",
            "There are 3 directories and 0 images in 'data/pizza_steak_sushi_full/test'.\n",
            "There are 0 directories and 250 images in 'data/pizza_steak_sushi_full/test/steak'.\n",
            "There are 0 directories and 250 images in 'data/pizza_steak_sushi_full/test/pizza'.\n",
            "There are 0 directories and 250 images in 'data/pizza_steak_sushi_full/test/sushi'.\n",
            "There are 3 directories and 0 images in 'data/pizza_steak_sushi_full/train'.\n",
            "There are 0 directories and 750 images in 'data/pizza_steak_sushi_full/train/steak'.\n",
            "There are 0 directories and 750 images in 'data/pizza_steak_sushi_full/train/pizza'.\n",
            "There are 0 directories and 750 images in 'data/pizza_steak_sushi_full/train/sushi'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Creating Datasets and Dataloaders"
      ],
      "metadata": {
        "id": "QebMIPAuLbDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Dirs \n",
        "\n",
        "#----------------------------------\n",
        "# small dataset\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "#----------------------------------\n",
        "#full dataset\n",
        "#train_dir = image_path_full / \"train\"\n",
        "#test_dir = image_path_full / \"test\""
      ],
      "metadata": {
        "id": "CYaIdpB-MtmJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Create DataLoaders using manually created transforms"
      ],
      "metadata": {
        "id": "I-M918PZMaVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train dir : \", train_dir)\n",
        "print(\"Test dir :\", test_dir)\n",
        "\n",
        "# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Create transform pipeline manually\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])           \n",
        "print(f\"Manually created transforms: {manual_transforms}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=32\n",
        ")\n",
        "print(F\"Training Images : {len(train_dataloader.dataset)}\")\n",
        "print(F\"Testing Images : {len(test_dataloader.dataset)}\")\n",
        "\n",
        "print(\"train_dataloader: \", train_dataloader)\n",
        "print(\"test_dataloader: \", test_dataloader)\n",
        "\n",
        "print(F\"Class names : {class_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlx3eSAtMc5W",
        "outputId": "f15b4efd-df49-4372-dd6d-2d090dedc124"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dir :  data/pizza_steak_sushi/train\n",
            "Test dir : data/pizza_steak_sushi/test\n",
            "Manually created transforms: Compose(\n",
            "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n",
            "Training Images : 225\n",
            "Testing Images : 75\n",
            "train_dataloader:  <torch.utils.data.dataloader.DataLoader object at 0x7fe5fa5a2320>\n",
            "test_dataloader:  <torch.utils.data.dataloader.DataLoader object at 0x7fe5fa5a1ed0>\n",
            "Class names : ['pizza', 'steak', 'sushi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Creating DataLoaders using automatically created transforms"
      ],
      "metadata": {
        "id": "B4R3VQtBPwnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train dir : \", train_dir)\n",
        "print(\"Test dir :\", test_dir, \"\\n\")\n",
        "\n",
        "# Setup pretrained weights (plenty of these available in torchvision.models)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "\n",
        "# Get transforms from weights (these are the transforms that were used to obtain the weights)\n",
        "automatic_transforms = weights.transforms() \n",
        "print(f\"Automatically created transforms: {automatic_transforms}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=automatic_transforms, # use automatic created transforms\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(F\"Training Images : {len(train_dataloader.dataset)}\")\n",
        "print(F\"Testing Images : {len(test_dataloader.dataset)}\")\n",
        "\n",
        "print(\"train_dataloader: \", train_dataloader)\n",
        "print(\"test_dataloader: \", test_dataloader)\n",
        "\n",
        "print(F\"Class names : {class_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPALPilAP039",
        "outputId": "812f58f9-a8b2-45ee-8205-1d99b03434e8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dir :  data/pizza_steak_sushi/train\n",
            "Test dir : data/pizza_steak_sushi/test \n",
            "\n",
            "Automatically created transforms: ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[256]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BICUBIC\n",
            ")\n",
            "Training Images : 225\n",
            "Testing Images : 75\n",
            "train_dataloader:  <torch.utils.data.dataloader.DataLoader object at 0x7fe5fa5a3dc0>\n",
            "test_dataloader:  <torch.utils.data.dataloader.DataLoader object at 0x7fe5fa5a29e0>\n",
            "Class names : ['pizza', 'steak', 'sushi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given transformation, the `crop_size` and `resize_size` parameters serve different purposes.\n",
        "\n",
        "1. `crop_size`: This parameter determines the size of the image after cropping. It specifies the dimensions (width and height) of the cropped image. During training or evaluation, the input image is typically cropped to a fixed size to ensure consistent input dimensions for the model. In this case, the `crop_size` is set to `[224]`, which means the cropped image will have a width and height of 224 pixels.\n",
        "\n",
        "2. `resize_size`: This parameter determines the size of the image after resizing. It specifies the dimensions (width and height) to which the image should be resized. Resizing is commonly performed to standardize the input image size for the model. In this case, the `resize_size` is set to `[256]`, indicating that the image will be resized to a width and height of 256 pixels.\n",
        "\n",
        "The purpose of using both `crop_size` and `resize_size` is to apply a two-step transformation to the input images. First, the image is resized to a larger size (`resize_size`). Then, a central crop of the specified size (`crop_size`) is extracted from the resized image.\n",
        "\n",
        "By applying this two-step transformation, the model receives inputs that have been both resized and cropped, ensuring that the model receives consistent input sizes and capturing different scales and perspectives of the original image.\n",
        "\n",
        "The other parameters (`mean`, `std`, `interpolation`) specify additional image transformation details such as mean normalization, standard deviation normalization, and the interpolation method used during resizing."
      ],
      "metadata": {
        "id": "8dlZqpUNYZiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Getting a pretrained model, freezing the base layers and changing the classifier head\n"
      ],
      "metadata": {
        "id": "shham8a7Qmix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # best weights available \n",
        "\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "\n",
        "#model"
      ],
      "metadata": {
        "id": "qE3rPywGQqdb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll freeze the base layers of the model (we'll use these to extract features from our input images) and we'll change the classifier head (output layer) to suit the number of classes we're working with (we've got 3 classes: pizza, steak, sushi)."
      ],
      "metadata": {
        "id": "R4lyPCaFRrdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeznig the base layers \n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False \n",
        "\n",
        "# Since we're creating a new layer with random weights (torch.nn.Linear), \n",
        "# let's set the seeds\n",
        "set_seeds()\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features=1280,\n",
        "              out_features=len(class_names),\n",
        "              bias=True)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "XUps1pmkRuzG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Get a summary of the model (uncomment for full output)\n",
        "summary(model, \n",
        "         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "         verbose=0,\n",
        "         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "         col_width=20,\n",
        "         row_settings=[\"var_names\"]\n",
        " )"
      ],
      "metadata": {
        "id": "Oh7k5W13VNB8",
        "outputId": "b1da8327-f1e5-4cc1-ff97-cb36f6bc7bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
              "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
              "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
              "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
              "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
              "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
              "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
              "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
              "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
              "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
              "============================================================================================================================================\n",
              "Total params: 4,011,391\n",
              "Trainable params: 3,843\n",
              "Non-trainable params: 4,007,548\n",
              "Total mult-adds (G): 12.31\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.09\n",
              "Params size (MB): 16.05\n",
              "Estimated Total Size (MB): 3487.41\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}